## Reframed Problem Statement

**The bet:** A conversational AI agent that probes, reflects, and synthesizes can make users feel more understood in 2-3 minutes than any wizard could — and that felt understanding translates to better habit recommendations and higher follow-through.

**What you're testing:**
1. Can the AI ask smart follow-up questions based on what the user says?
2. Can the AI demonstrate insight (not just collect data)?
3. Can the AI synthesize a personalized system that feels "made for me"?
4. Does this feel like co-design, not configuration?

**What you're NOT testing:**
- Whether wizards can be optimized
- Whether playbooks scale
- Whether the app works for 1000 users

This is an AI capabilities test wrapped in a product.

---

## What the Intake Agent Needs to Do

Based on your answers (D, B+C, context-aware, wizard risk), here's the job spec for the intake agent:

### Primary Jobs
1. **Understand the goal** — What does the user actually want? (Not which bucket they fit in)
2. **Probe for context** — What's their life situation, constraints, history with this?
3. **Demonstrate insight** — Show the user you understand their *pattern*, not just their words
4. **Surface the real blocker** — Often what people say they want isn't the actual leverage point

### Conversation Mechanics
- Ask one question at a time
- Offer suggested responses (pills) but always allow free-text
- Reflect back understanding before moving on
- Share relevant insight that earns trust ("Most people who want to wake earlier fail because...")
- Know when you have enough to make a recommendation

### Output
By end of conversation, the agent should have:
- Clear understanding of the user's goal (in their words, not a category)
- Key constraints (timing, blockers, history)
- A hypothesis about the real leverage point
- Enough context to generate a personalized habit + system

---

## Key Design Questions for the Chat Agent

### 1. How does it know when to stop asking and start recommending?

Options:
- **Token limit** — After N exchanges, synthesize and move on
- **Coverage check** — Agent tracks what it knows (goal, blocker, timing, history) and asks until it has all four
- **User signal** — User can say "I think you have enough" or tap "Ready for recommendations"
- **AI judgment** — Agent decides when it has sufficient context

**My recommendation:** Coverage check + user escape hatch. Agent has a mental checklist, but user can accelerate.

### 2. How structured are the suggested responses?

Options:
- **Fully dynamic** — AI generates pill suggestions based on conversation
- **Semi-structured** — Certain questions have fixed pills, others are open
- **Contextual templates** — Pills are pulled from a library based on detected domain

**My recommendation:** Start with semi-structured. Key questions (what's the blocker? tried before?) have pre-defined pills because they're reliably useful. But the opening question and follow-ups are AI-generated.

### 3. Where does domain knowledge come in?

You said no pre-configured playbooks, but the AI still needs to know things like:
- "Most sleep problems are actually bedtime problems, not wake-up problems"
- "The highest-leverage exercise habit is usually the smallest one"
- "Financial habits fail when they require daily willpower"

**Question:** Is this knowledge embedded in the system prompt, or does the AI retrieve it dynamically, or does it just use its training?

**My recommendation:** System prompt with domain principles (not rigid playbooks). The AI knows *frameworks* for thinking about sleep/exercise/finance but doesn't have fixed habit recommendations. It generates those based on the conversation.

### 4. What's the handoff to Portrait/Habits?

Current flow: Chat → Portrait → Habits → System → Contract

If chat does its job well, do you still need Portrait? Or does the insight-sharing happen *within* the chat?

**Option A:** Chat handles everything through Habits recommendation. Portrait is eliminated — its content gets woven into the conversation.

**Option B:** Chat handles intake, then Portrait is a *summary* of what the AI learned, not a lecture. "Based on what you told me, here's what I know about your situation..."

**My recommendation:** Option A for the experiment. If the chat is working, Portrait becomes redundant. You can always add it back if something's missing.

---

## Proposed Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    INTAKE AGENT                         │
│                                                         │
│  System prompt:                                         │
│  - Role: Habit design coach                             │
│  - Goal: Understand user's situation deeply             │
│  - Principles: Ask one Q at a time, reflect back,       │
│    share insight to earn trust, probe for real blocker  │
│  - Domain knowledge: Frameworks for sleep, exercise,    │
│    finance, etc. (not rigid playbooks)                  │
│  - Exit criteria: Have goal, blocker, timing, history   │
│                                                         │
│  Conversation state:                                    │
│  - goal: string | null                                  │
│  - blocker: string | null                               │
│  - timing: string | null                                │
│  - history: string | null                               │
│  - additional_context: string[]                         │
│                                                         │
│  Each turn:                                             │
│  1. Process user input                                  │
│  2. Update conversation state                           │
│  3. Decide: ask more, reflect, share insight, or exit   │
│  4. Generate response + suggested pills                 │
│                                                         │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────┐
│                 RECOMMENDATION AGENT                    │
│                                                         │
│  Input: Conversation state from intake                  │
│                                                         │
│  Output:                                                │
│  - 2-3 personalized habit recommendations               │
│  - For each: why it fits THIS user                      │
│  - Suggested anchor based on their timing/constraints   │
│  - Recovery action                                      │
│                                                         │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────┐
│                    SYSTEM DESIGN                        │
│  (User selects habit, confirms anchor, sees contract)   │
└─────────────────────────────────────────────────────────┘
```

---

## Next Steps — What I'd Recommend

### Step 1: Write the Intake Agent System Prompt
Before building UI, write the prompt that defines how this agent behaves. Test it in Claude directly with various user inputs. Does it ask good follow-ups? Does it know when to stop? Does it share insight?

### Step 2: Define the Conversation State Schema
What does the agent need to extract? Goal, blocker, timing, history, constraints? Define this clearly so you know what "done" looks like.

### Step 3: Build Minimal Chat UI
Single screen: chat bubbles + text input + suggested pills. No progress stepper. The agent controls the flow.

### Step 4: Test with Your Sleep Goal
Run through it yourself. Does it feel like co-design? Does it earn the right to recommend? Where does it break?

---